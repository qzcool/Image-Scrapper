{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Media Scrapper\n",
        "## Glossary\n",
        "1. Story: one topic of images that often displays on multiple pages\n",
        "\n",
        "## Goal: *Download the best story under a certain tag automatically.*\n",
        "\n",
        "## Procedures\n",
        "1. Create a list of story urls and popularity by tag, with story urls descending ordered by popularity\n",
        "2. Test if story url is downloadable\n",
        "3. Get story name to create a folder for the incoming images\n",
        "4. Confirm series of pages for the same story\n",
        "5. Download all images given a single story url, including multiple pages under a story\n",
        "6. Save all images to the folder under the story\n",
        "\n",
        "## Precautions\n",
        "1. Scrapper interval: time.sleep(0.5)\n",
        "2. FakeAgent: fake browser login\n",
        "\n",
        "## Image source\n",
        "1. http://www.bfpgf.com/fsh/82835.html/\n",
        "2. http://93.t9p.today/forumdisplay.php?fid=19&page=1\n",
        "3. Pinterest\n",
        "4. Instagram\n",
        "5. Weibo\n",
        "\n",
        "## Improvements\n",
        "- [x] How to add image number information to the story folder suffix?\n",
        "- [ ] Update the Tag Folder Reguarly, Show lastest images\n",
        "- [ ] Popularity Trend of Story\n",
        "- [ ] Download all images of a story on multiple pages (for competition or poll story)\n",
        "- [ ] Given any url, download all images on that webpage, save to a given directory\n",
        "- [ ] Download images from all image sources under the same tag\n",
        "- [ ] Daily newsfeed for subscribers\n",
        "- [ ] Performance Enhancement: Better move file\n",
        "- [ ] Facial recognition by dlib to classify different stars\n",
        "- [ ] Build as an API\n",
        "- [ ] Web application: Django + Vue.JS"
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fuligets Scrapper"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Fuligets Scrapper\n",
        "# http://www.bfpgf.com/\n",
        "\n",
        "# Input a full URL Address\n",
        "url = 'http://www.bfpgf.com/yld/82962.html'\n",
        "\n",
        "######################\n",
        "# Don't Change Below #\n",
        "######################\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "from fake_useragent import UserAgent\n",
        "from tqdm import *\n",
        "import requests, os, datetime, shutil, re, time\n",
        "\n",
        "# Download File Common Function\n",
        "def download_file(url, path):\n",
        "    local_filename = url.split('/')[-1]\n",
        "    # NOTE the stream=True parameter\n",
        "    r = requests.get(url, stream=True)\n",
        "    with open(local_filename, 'wb') as f:\n",
        "        for chunk in r.iter_content(chunk_size=1024): \n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "                #f.flush() commented by recommendation from J.F.Sebastian\n",
        "    shutil.move(local_filename,path+'/'+local_filename)\n",
        "\n",
        "# Get Story Name\n",
        "#headers = {'User-Agent': str(UserAgent().chrome)}\n",
        "r = requests.get(url)#,headers)\n",
        "soup = BeautifulSoup(r.text,'lxml')\n",
        "name = soup.find('h1',class_=\"article-title\").find('a').get_text()\n",
        "\n",
        "# Create Folder\n",
        "os.makedirs(name)\n",
        "\n",
        "# Confirm Pages\n",
        "if len(soup.find_all('div',class_='wp-pagenavi')) == 1:\n",
        "    comments=soup.find_all(string=lambda text:isinstance(text,Comment))\n",
        "    for c in comments:\n",
        "        if url in c:\n",
        "            text = str(c)\n",
        "            pages = int(re.compile(\".*<span>(.*)</span>.*\").match(text).group(1))\n",
        "        else: \n",
        "            pass\n",
        "else: \n",
        "    pages = 1\n",
        "        \n",
        "# Download Images\n",
        "for i in tqdm(range(1,pages+1,1)):\n",
        "    url_idv = url+'/'+str(i)\n",
        "    headers = {'User-Agent': str(UserAgent().chrome)}\n",
        "\n",
        "    r = requests.get(url_idv,headers)\n",
        "    soup = BeautifulSoup(r.text,'lxml')\n",
        "    for img in soup.find('article',class_='article-content').find_all('img'):\n",
        "        download_file('http://www.bfpgf.com'+img['src'],name)\n",
        "    time.sleep(0.01)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Error occurred during loading data. Trying to use cache server https://fake-useragent.herokuapp.com/browsers/0.1.8\n",
            "Traceback (most recent call last):\n",
            "  File \"/anaconda/lib/python3.6/urllib/request.py\", line 1318, in do_open\n",
            "    encode_chunked=req.has_header('Transfer-encoding'))\n",
            "  File \"/anaconda/lib/python3.6/http/client.py\", line 1239, in request\n",
            "    self._send_request(method, url, body, headers, encode_chunked)\n",
            "  File \"/anaconda/lib/python3.6/http/client.py\", line 1285, in _send_request\n",
            "    self.endheaders(body, encode_chunked=encode_chunked)\n",
            "  File \"/anaconda/lib/python3.6/http/client.py\", line 1234, in endheaders\n",
            "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
            "  File \"/anaconda/lib/python3.6/http/client.py\", line 1026, in _send_output\n",
            "    self.send(msg)\n",
            "  File \"/anaconda/lib/python3.6/http/client.py\", line 964, in send\n",
            "    self.connect()\n",
            "  File \"/anaconda/lib/python3.6/http/client.py\", line 1392, in connect\n",
            "    super().connect()\n",
            "  File \"/anaconda/lib/python3.6/http/client.py\", line 936, in connect\n",
            "    (self.host,self.port), self.timeout, self.source_address)\n",
            "  File \"/anaconda/lib/python3.6/socket.py\", line 724, in create_connection\n",
            "    raise err\n",
            "  File \"/anaconda/lib/python3.6/socket.py\", line 713, in create_connection\n",
            "    sock.connect(sa)\n",
            "socket.timeout: timed out\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/anaconda/lib/python3.6/site-packages/fake_useragent/utils.py\", line 67, in get\n",
            "    context=context,\n",
            "  File \"/anaconda/lib/python3.6/urllib/request.py\", line 223, in urlopen\n",
            "    return opener.open(url, data, timeout)\n",
            "  File \"/anaconda/lib/python3.6/urllib/request.py\", line 526, in open\n",
            "    response = self._open(req, data)\n",
            "  File \"/anaconda/lib/python3.6/urllib/request.py\", line 544, in _open\n",
            "    '_open', req)\n",
            "  File \"/anaconda/lib/python3.6/urllib/request.py\", line 504, in _call_chain\n",
            "    result = func(*args)\n",
            "  File \"/anaconda/lib/python3.6/urllib/request.py\", line 1361, in https_open\n",
            "    context=self._context, check_hostname=self._check_hostname)\n",
            "  File \"/anaconda/lib/python3.6/urllib/request.py\", line 1320, in do_open\n",
            "    raise URLError(err)\n",
            "urllib.error.URLError: <urlopen error timed out>\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/anaconda/lib/python3.6/site-packages/fake_useragent/utils.py\", line 154, in load\n",
            "    for item in get_browsers(verify_ssl=verify_ssl):\n",
            "  File \"/anaconda/lib/python3.6/site-packages/fake_useragent/utils.py\", line 97, in get_browsers\n",
            "    html = get(settings.BROWSERS_STATS_PAGE, verify_ssl=verify_ssl)\n",
            "  File \"/anaconda/lib/python3.6/site-packages/fake_useragent/utils.py\", line 84, in get\n",
            "    raise FakeUserAgentError('Maximum amount of retries reached')\n",
            "fake_useragent.errors.FakeUserAgentError: Maximum amount of retries reached\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false,
        "outputExpanded": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 91 Scrapper: Image"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 91 Scrapper: Image\n",
        "# http://93.t9p.today/index.php\n",
        "\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "from fake_useragent import UserAgent\n",
        "from tqdm import *\n",
        "import requests, os, datetime, shutil, re, time\n",
        "import pandas as pd\n",
        "\n",
        "# Create a list of story urls by certain tags, ranked by popularity, only keeped tops of the list and undownloadable excluded\n",
        "def get_stories_by_tag(tag, top_number):\n",
        "    # Use 'try-finally' in case the input tag does not exist\n",
        "    try:\n",
        "        # Get totol pages number of a tag \n",
        "        url = 'http://93.t9p.today/tag.php?name='+tag\n",
        "        headers = {'User-Agent': str(UserAgent().chrome)}\n",
        "        r = requests.get(url,headers)\n",
        "        r.encoding = 'utf-8'\n",
        "        soup = BeautifulSoup(r.text,'lxml')\n",
        "        pages = 1\n",
        "        for page in soup.find('div',class_='pages_btns').find_all('a',class_=False):\n",
        "            pages += 1\n",
        "        \n",
        "        # Create a list (dataframe) of story names, popularity and story urls\n",
        "        df_urls = pd.DataFrame(columns=['name','popularity','url','image_number'])\n",
        "        for page in tqdm(range(pages)):\n",
        "            url = 'http://93.t9p.today/tag.php?name='+tag+'&page='+str(page)\n",
        "            headers = {'User-Agent': str(UserAgent().chrome)}\n",
        "            r = requests.get(url,headers)\n",
        "            r.encoding = 'utf-8'\n",
        "            soup_tag = BeautifulSoup(r.text,'lxml')\n",
        "            # Get all available stories with url and popularity data and append them into the DataFrame\n",
        "            for story in soup_tag.find_all('tbody'):\n",
        "                url = 'http://93.t9p.today/'+story.find('a')['href']\n",
        "                headers = {'User-Agent': str(UserAgent().chrome)}\n",
        "                r = requests.get(url,headers)\n",
        "                r.encoding = 'utf-8'\n",
        "                soup_url = BeautifulSoup(r.text,'lxml')\n",
        "                # Filter unaccessible stories\n",
        "                if soup_url.find('div',class_='alert_error') or soup_url.find('div',class_='postmessage firstpost').find('div',class_='locked') or (len(soup_url.find_all('img',file=True,width=True,id=True,alt=True)) == 0):\n",
        "                    pass\n",
        "                else:\n",
        "                    df_urls = df_urls.append({'name':story.find('a').get_text(), 'popularity':int(story.find('td',class_='nums').find('em').get_text()), 'url':'http://93.t9p.today/'+story.find('a')['href'],'image_number':len(soup_url.find_all('img',file=True,width=True,id=True,alt=True))}, ignore_index=True)\n",
        "        # Order by popularity, only keep the top_number of stories\n",
        "        df_urls = df_urls.drop_duplicates(subset='url').sort_values(by='popularity',ascending=False).iloc[:top_number,:]\n",
        "        # 移除含有删帖字样的帖子\n",
        "        df_urls['标题是否含删帖'] = df_urls['name'].apply(lambda x:'删帖' in x)\n",
        "        df_urls = df_urls[df_urls['标题是否含删帖'] == False].iloc[:,:-1]\n",
        "        df_urls.index = range(len(df_urls))\n",
        "        df_urls.to_csv('91论坛'+tag+'精选.csv', encoding='gb18030')\n",
        "    \n",
        "    except: \n",
        "        print ('The input tag does not exist. Please input a new tag.')\n",
        "\n",
        "image_number_total =0\n",
        "\n",
        "# Download all images of a story and save them to one folder on desktop\n",
        "def download_story(url):\n",
        "    # Download File Common Function\n",
        "    def download_file(url, path):\n",
        "        local_filename = url.split('/')[-1]\n",
        "        # NOTE the stream=True parameter\n",
        "        r = requests.get(url, stream=True)\n",
        "        with open(local_filename, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=1024): \n",
        "                if chunk: # filter out keep-alive new chunks\n",
        "                    f.write(chunk)\n",
        "                    #f.flush() commented by recommendation from J.F.Sebastian\n",
        "        shutil.move(local_filename,path+'/'+local_filename)\n",
        "\n",
        "    # Get Story Name\n",
        "    headers = {'User-Agent': str(UserAgent().chrome)}\n",
        "    r = requests.get(url,headers)\n",
        "    r.encoding = 'utf-8'\n",
        "    soup = BeautifulSoup(r.text,'lxml')\n",
        "    name = soup.find('title').get_text().split('-')[0].strip()\n",
        "\n",
        "    # Create Folder, only when there is at least one image in the story\n",
        "    os.makedirs(name)\n",
        "    \n",
        "    # Download Images and Count Numbers of Images\n",
        "    image_number = 0\n",
        "    for img in soup.find_all('img',file=True,width=True,id=True,alt=True):\n",
        "        image_number += 1\n",
        "        download_file('http://93.t9p.today/'+img['file'],name)\n",
        "        time.sleep(0.05) # Scrapping too fast will cause server error. Immortal! Inhonorable!\n",
        "    # Rename Folder and Add Image Counts Suffix\n",
        "    os.rename(name,name+'['+str(image_number)+'P]')\n",
        "    \n",
        "    global image_number_total\n",
        "    image_number_total += image_number"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 91 Scrapper: Image\n",
        "# Input a Tag of your interests, with top input numbers of stories displayed.\n",
        "# Images will be saved in folders under the story name separately at the path where this Jupyter Notebook file locates.\n",
        "\n",
        "##### INPUT AREA ######\n",
        "tag_input = '模特' # Name the tag you want\n",
        "top_story_num_input = 5 # 'None': for all stories; 5: for top 5 stories of the tag\n",
        "#######################\n",
        "\n",
        "get_stories_by_tag(tag_input,top_story_num_input)\n",
        "\n",
        "list_urls = pd.read_csv('91论坛'+tag_input+'精选.csv',encoding='gb18030').url.tolist()\n",
        "for url in tqdm(list_urls):\n",
        "    try:\n",
        "        download_story(url)\n",
        "    except Exception as e:\n",
        "        print (str(e), url)\n",
        "\n",
        "# Total Numbers of Images under a Tag\n",
        "print (tag_input,'标签共有图片：',image_number_total)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 91 Scrapper: Video"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 91 Scrapper: Video\n",
        "\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "from fake_useragent import UserAgent\n",
        "from tqdm import *\n",
        "import requests, os, datetime, shutil, re, time\n",
        "import pandas as pd\n",
        "\n",
        "# Download File Common Function\n",
        "def download_file(url):\n",
        "    local_filename = url.split('/')[-1]\n",
        "    # NOTE the stream=True parameter\n",
        "    r = requests.get(url, stream=True)\n",
        "    with open(local_filename, 'wb') as f:\n",
        "        for chunk in r.iter_content(chunk_size=1024): \n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "def get_video_url(url):\n",
        "    # Get the video url from the video's page url\n",
        "    headers = {'User-Agent': str(UserAgent().chrome)}\n",
        "    r = requests.get(url,headers)\n",
        "    r.encoding = 'utf-8'\n",
        "    soup = BeautifulSoup(r.text,'lxml')\n",
        "    soup.find('source', src=True, type='video/mp4')"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_video_url('http://91porn.com/view_video.php?viewkey=49d446466df6c7912edb&page=1&viewtype=basic&category=hot')"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'http://91porn.com/view_video.php?viewkey=49d446466df6c7912edb&page=1&viewtype=basic&category=hot'\n",
        "\n",
        "# Get the video url from the video's page url\n",
        "headers = {'User-Agent': str(UserAgent().chrome)}\n",
        "r = requests.get(url,headers)\n",
        "r.encoding = 'utf-8'\n",
        "soup = BeautifulSoup(r.text,'lxml')\n",
        "soup.find('source', src=True, type='video/mp4')"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "download_file('http://185.38.13.159//mp43/253097.mp4?st=oxFGYsXz1lcmhpHOpr8-8Q&e=1520023447')"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}