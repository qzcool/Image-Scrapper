{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Image Scrapper\n",
        "## Glossary\n",
        "1. Story: one topic of images that often displays on multiple pages\n",
        "\n",
        "## Goal: *Download the best story under a certain tag automatically.*\n",
        "\n",
        "## Procedures\n",
        "1. Create a list of story urls and popularity by tag, with story urls descending ordered by popularity\n",
        "2. Test if story url is downloadable\n",
        "3. Get story name to create a folder for the incoming images\n",
        "4. Confirm series of pages for the same story\n",
        "5. Download all images given a single story url, including multiple pages under a story\n",
        "6. Save all images to the folder under the story\n",
        "\n",
        "## Precautions\n",
        "1. Scrapper interval: time.sleep(0.5)\n",
        "2. FakeAgent: fake browser login\n",
        "\n",
        "## Image source\n",
        "1. http://www.bfpgf.com/yld/82777.html\n",
        "2. http://93.t9p.today/forumdisplay.php?fid=19&page=1\n",
        "\n",
        "## Improvements\n",
        "- [x] How to add image number information to the story folder suffix?\n",
        "- [ ] Update the Tag Folder Reguarly, Show lastest images\n",
        "- [ ] Popularity Trend of Story\n",
        "- [ ] Download all images of a story on multiple pages (for competition or poll story)"
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fuligets Scrapper"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Fuligets Scrapper\n",
        "# http://www.bfpgf.com/\n",
        "\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "from fake_useragent import UserAgent\n",
        "from tqdm import *\n",
        "import requests, os, datetime, shutil, re, time\n",
        "\n",
        "# Download File Common Function\n",
        "def download_file(url, path):\n",
        "    local_filename = url.split('/')[-1]\n",
        "    # NOTE the stream=True parameter\n",
        "    r = requests.get(url, stream=True)\n",
        "    with open(local_filename, 'wb') as f:\n",
        "        for chunk in r.iter_content(chunk_size=1024): \n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "                #f.flush() commented by recommendation from J.F.Sebastian\n",
        "    shutil.move(local_filename,path+'/'+local_filename)\n",
        "\n",
        "# Input URL Name\n",
        "url = 'http://www.bfpgf.com/yld/83152.html'\n",
        "\n",
        "# Get Story Name\n",
        "headers = {'User-Agent': str(UserAgent().chrome)}\n",
        "r = requests.get(url,headers)\n",
        "soup = BeautifulSoup(r.text,'lxml')\n",
        "name = soup.find('h1',class_=\"article-title\").find('a').get_text()\n",
        "\n",
        "# Create Folder\n",
        "os.makedirs(name)\n",
        "\n",
        "# Confirm Pages\n",
        "comments=soup.find_all(string=lambda text:isinstance(text,Comment))\n",
        "for c in comments:\n",
        "    if url in c:\n",
        "        text = str(c)\n",
        "        pages = int(re.compile(\".*<span>(.*)</span>.*\").match(text).group(1))\n",
        "    else: \n",
        "        pass\n",
        "    \n",
        "# Download Images\n",
        "for i in tqdm(range(1,pages+1,1)):\n",
        "    url_idv = url+'/'+str(i)\n",
        "    headers = {'User-Agent': str(UserAgent().chrome)}\n",
        "\n",
        "    r = requests.get(url_idv,headers)\n",
        "    soup = BeautifulSoup(r.text,'lxml')\n",
        "    for img in soup.find('article',class_='article-content').find_all('img'):\n",
        "        download_file('http://www.bfpgf.com'+img['src'],name)\n",
        "    time.sleep(0.05)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false,
        "outputExpanded": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 91 Scrapper"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# 91 Scrapper\n",
        "# http://93.t9p.today/index.php\n",
        "\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "from fake_useragent import UserAgent\n",
        "from tqdm import *\n",
        "import requests, os, datetime, shutil, re, time\n",
        "import pandas as pd\n",
        "\n",
        "# Create a list of story urls by certain tags, ranked by popularity, only keeped tops of the list and undownloadable excluded\n",
        "def get_stories_by_tag(tag, top_number):\n",
        "    # Use 'try-finally' in case the input tag does not exist\n",
        "    try:\n",
        "        # Get totol pages number of a tag \n",
        "        url = 'http://93.t9p.today/tag.php?name='+tag\n",
        "        headers = {'User-Agent': str(UserAgent().chrome)}\n",
        "        r = requests.get(url,headers)\n",
        "        r.encoding = 'utf-8'\n",
        "        soup = BeautifulSoup(r.text,'lxml')\n",
        "        pages = 1\n",
        "        for page in soup.find('div',class_='pages_btns').find_all('a',class_=False):\n",
        "            pages += 1\n",
        "        \n",
        "        # Create a list (dataframe) of story names, popularity and story urls\n",
        "        df_urls = pd.DataFrame(columns=['name','popularity','url','image_number'])\n",
        "        for page in tqdm(range(pages)):\n",
        "            url = 'http://93.t9p.today/tag.php?name='+tag+'&page='+str(page)\n",
        "            headers = {'User-Agent': str(UserAgent().chrome)}\n",
        "            r = requests.get(url,headers)\n",
        "            r.encoding = 'utf-8'\n",
        "            soup_tag = BeautifulSoup(r.text,'lxml')\n",
        "            # Get all available stories with url and popularity data and append them into the DataFrame\n",
        "            for story in soup_tag.find_all('tbody'):\n",
        "                url = 'http://93.t9p.today/'+story.find('a')['href']\n",
        "                headers = {'User-Agent': str(UserAgent().chrome)}\n",
        "                r = requests.get(url,headers)\n",
        "                r.encoding = 'utf-8'\n",
        "                soup_url = BeautifulSoup(r.text,'lxml')\n",
        "                # Filter unaccessible stories\n",
        "                if soup_url.find('div',class_='alert_error') or soup_url.find('div',class_='postmessage firstpost').find('div',class_='locked') or (len(soup_url.find_all('img',file=True,width=True,id=True,alt=True)) == 0):\n",
        "                    pass\n",
        "                else:\n",
        "                    df_urls = df_urls.append({'name':story.find('a').get_text(), 'popularity':int(story.find('td',class_='nums').find('em').get_text()), 'url':'http://93.t9p.today/'+story.find('a')['href'],'image_number':len(soup_url.find_all('img',file=True,width=True,id=True,alt=True))}, ignore_index=True)\n",
        "        # Order by popularity, only keep the top_number of stories\n",
        "        df_urls = df_urls.drop_duplicates(subset='url').sort_values(by='popularity',ascending=False).iloc[:top_number,:]\n",
        "        # 移除含有删帖字样的帖子\n",
        "        df_urls['标题是否含删帖'] = df_urls['name'].apply(lambda x:'删帖' in x)\n",
        "        df_urls = df_urls[df_urls['标题是否含删帖'] == False].iloc[:,:-1]\n",
        "        df_urls.index = range(len(df_urls))\n",
        "        df_urls.to_csv('91论坛'+tag+'精选.csv', encoding='gb18030')\n",
        "    \n",
        "    except: \n",
        "        print ('The input tag does not exist. Please input a new tag.')\n",
        "\n",
        "image_number_total =0\n",
        "\n",
        "# Download all images of a story and save them to one folder on desktop\n",
        "def download_story(url):\n",
        "    # Download File Common Function\n",
        "    def download_file(url, path):\n",
        "        local_filename = url.split('/')[-1]\n",
        "        # NOTE the stream=True parameter\n",
        "        r = requests.get(url, stream=True)\n",
        "        with open(local_filename, 'wb') as f:\n",
        "            for chunk in r.iter_content(chunk_size=1024): \n",
        "                if chunk: # filter out keep-alive new chunks\n",
        "                    f.write(chunk)\n",
        "                    #f.flush() commented by recommendation from J.F.Sebastian\n",
        "        shutil.move(local_filename,path+'/'+local_filename)\n",
        "\n",
        "    # Get Story Name\n",
        "    headers = {'User-Agent': str(UserAgent().chrome)}\n",
        "    r = requests.get(url,headers)\n",
        "    r.encoding = 'utf-8'\n",
        "    soup = BeautifulSoup(r.text,'lxml')\n",
        "    name = soup.find('title').get_text().split('-')[0].strip()\n",
        "\n",
        "    # Create Folder, only when there is at least one image in the story\n",
        "    os.makedirs(name)\n",
        "    \n",
        "    # Download Images and Count Numbers of Images\n",
        "    image_number = 0\n",
        "    for img in soup.find_all('img',file=True,width=True,id=True,alt=True):\n",
        "        image_number += 1\n",
        "        download_file('http://93.t9p.today/'+img['file'],name)\n",
        "        time.sleep(0.05) # Scrapping too fast will cause server error. Immortal! Inhonorable!\n",
        "    # Rename Folder and Add Image Counts Suffix\n",
        "    os.rename(name,name+'['+str(image_number)+'P]')\n",
        "    \n",
        "    global image_number_total\n",
        "    image_number_total += image_number"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input a Tag of your interests, with top input numbers of stories displayed.\n",
        "# Images will be saved in folders under the story name separately at the path where this Jupyter Notebook file locates.\n",
        "\n",
        "tag_input = '模特' # Name the tag you want\n",
        "top_story_num_input = 5 # 'None': for all stories; 5: for top 5 stories of the tag\n",
        "\n",
        "get_stories_by_tag(tag_input,top_story_num_input)\n",
        "\n",
        "list_urls = pd.read_csv('91论坛'+tag_input+'精选.csv',encoding='gb18030').url.tolist()\n",
        "for url in tqdm(list_urls):\n",
        "    try:\n",
        "        download_story(url)\n",
        "    except Exception as e:\n",
        "        print (str(e), url)\n",
        "\n",
        "# Total Numbers of Images under a Tag\n",
        "print (tag_input,'标签共有图片：',image_number_total)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}